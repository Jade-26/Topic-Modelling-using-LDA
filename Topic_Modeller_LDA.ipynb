{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HQ0gxSfq_l9G"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'spacy'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3756/841754267.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m       \u001b[1;31m#regular expression is used to search the words in a text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wordnet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
          ]
        }
      ],
      "source": [
        "# importing libraries \n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re       #regular expression is used to search the words in a text\n",
        "import spacy\n",
        "import nltk \n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords  #stopwords means those words which doesn't add much value to text context like articles\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.4.2-cp39-cp39-win_amd64.whl (11.9 MB)\n",
            "     ---------------------------------------- 11.9/11.9 MB 3.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\rajat\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.21.2)\n",
            "Collecting thinc<8.2.0,>=8.1.0\n",
            "  Downloading thinc-8.1.5-cp39-cp39-win_amd64.whl (1.3 MB)\n",
            "     ---------------------------------------- 1.3/1.3 MB 3.4 MB/s eta 0:00:00\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.10\n",
            "  Downloading spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB)\n",
            "Collecting jinja2\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "     -------------------------------------- 133.1/133.1 kB 4.0 MB/s eta 0:00:00\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.2-py3-none-any.whl (42 kB)\n",
            "     ---------------------------------------- 42.8/42.8 kB 2.0 MB/s eta 0:00:00\n",
            "Collecting murmurhash<1.1.0,>=0.28.0\n",
            "  Downloading murmurhash-1.0.9-cp39-cp39-win_amd64.whl (18 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
            "  Downloading pydantic-1.10.2-cp39-cp39-win_amd64.whl (2.1 MB)\n",
            "     ---------------------------------------- 2.1/2.1 MB 3.2 MB/s eta 0:00:00\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\rajat\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.28.1)\n",
            "Collecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.5-cp39-cp39-win_amd64.whl (481 kB)\n",
            "     -------------------------------------- 481.4/481.4 kB 3.8 MB/s eta 0:00:00\n",
            "Collecting cymem<2.1.0,>=2.0.2\n",
            "  Downloading cymem-2.0.7-cp39-cp39-win_amd64.whl (30 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\rajat\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (21.3)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\rajat\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: setuptools in c:\\program files\\python39\\lib\\site-packages (from spacy) (57.4.0)\n",
            "Collecting preshed<3.1.0,>=3.0.2\n",
            "  Downloading preshed-3.0.8-cp39-cp39-win_amd64.whl (96 kB)\n",
            "     ---------------------------------------- 96.8/96.8 kB ? eta 0:00:00\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "     -------------------------------------- 181.6/181.6 kB 3.6 MB/s eta 0:00:00\n",
            "Collecting wasabi<1.1.0,>=0.9.1\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\rajat\\appdata\\roaming\\python\\python39\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Collecting smart-open<6.0.0,>=5.2.1\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "     ---------------------------------------- 58.6/58.6 kB ? eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\rajat\\appdata\\roaming\\python\\python39\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rajat\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rajat\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.12)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\rajat\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rajat\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
            "Collecting confection<1.0.0,>=0.0.1\n",
            "  Downloading confection-0.0.3-py3-none-any.whl (32 kB)\n",
            "Collecting blis<0.8.0,>=0.7.8\n",
            "  Downloading blis-0.7.9-cp39-cp39-win_amd64.whl (7.0 MB)\n",
            "     ---------------------------------------- 7.0/7.0 MB 3.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: colorama in c:\\users\\rajat\\appdata\\roaming\\python\\python39\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\rajat\\appdata\\roaming\\python\\python39\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rajat\\appdata\\roaming\\python\\python39\\site-packages (from jinja2->spacy) (2.1.1)\n",
            "Installing collected packages: wasabi, cymem, spacy-loggers, spacy-legacy, smart-open, pydantic, murmurhash, langcodes, jinja2, catalogue, blis, typer, srsly, preshed, pathy, confection, thinc, spacy\n",
            "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.3 cymem-2.0.7 jinja2-3.1.2 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.6.2 preshed-3.0.8 pydantic-1.10.2 smart-open-5.2.1 spacy-3.4.2 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.5 thinc-8.1.5 typer-0.4.2 wasabi-0.10.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The script pathy.exe is installed in 'C:\\Users\\Rajat\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script spacy.exe is installed in 'C:\\Users\\Rajat\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "\n",
            "[notice] A new release of pip available: 22.2.2 -> 22.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys2GCAH-8si-"
      },
      "source": [
        "### Importing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6Ie4dzqdZ2y"
      },
      "outputs": [],
      "source": [
        "  #useful in creaing dataframs and storing the data in the dataframes\n",
        "data  = pd.read_csv(\"https://github.com/srivatsan88/YouTubeLI/blob/master/dataset/consumer_compliants.zip?raw=true\",compression='zip')\n",
        "data.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMAxpTJz81wy"
      },
      "source": [
        "### Exploratory data analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67pd4OXnnByC"
      },
      "outputs": [],
      "source": [
        "data['Product'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrGBKFH3nSSg"
      },
      "outputs": [],
      "source": [
        "data['Company'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vybq7sg1yCn4"
      },
      "outputs": [],
      "source": [
        "# considering usefull data columns (Product,company,Consumer complaint narrative)\n",
        "\n",
        "complaint_data = data[['Product','Company','Consumer complaint narrative']].rename(columns={'Consumer complaint narrative':'Complaint'})\n",
        "\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "complaint_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNVF94NT3fAz"
      },
      "outputs": [],
      "source": [
        "# more exploration on dataset\n",
        "print(complaint_data.isnull().count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BztzPrSiypN0"
      },
      "outputs": [],
      "source": [
        "print(complaint_data.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxZa7X9y9BJT"
      },
      "source": [
        "### **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RgSsCIF3qUO"
      },
      "outputs": [],
      "source": [
        "# data set split for further analysis \n",
        "\n",
        "train,test = train_test_split(complaint_data,test_size=0.3,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQ2p2Cms4tDf"
      },
      "outputs": [],
      "source": [
        "train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5X3tkRJg4vQN"
      },
      "outputs": [],
      "source": [
        "\n",
        "train['Complaint_processed'] = train['Complaint'].map(lambda x: re.sub('[,\\.!?]', '', x))  #(˄ means exclusion of everything which is not alphabets and replacing it with null space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSyvyKjj570H"
      },
      "outputs": [],
      "source": [
        "train['Complaint_processed'] = train['Complaint_processed'].map(lambda x: x.lower())   #converting all the letters into lowercase\n",
        "train['Complaint_processed'].head()   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5Hi9Bb2B1PD"
      },
      "outputs": [],
      "source": [
        "!pip install nltk   #natural language toolkit "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l03lbnikqGTp"
      },
      "outputs": [],
      "source": [
        "stop_words = (stopwords.words('english'))\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmer = nltk.stem.SnowballStemmer('english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0XJhz7nBngL"
      },
      "source": [
        "## Tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XjRJ8Ut_36_"
      },
      "outputs": [],
      "source": [
        "\n",
        "def tokenize(text):\n",
        "   tokens = [word for word in nltk.word_tokenize(text) if (len(word) > 3 and len(word.strip('Xx/')) > 2 and len(re.sub('\\d+', '', word.strip('Xx/'))) > 3) ] \n",
        "  #  tokens = map(str.lower, tokens)\n",
        "   stems = [stemmer.stem(item) for item in tokens if (item not in stop_words)]\n",
        "   return stems\n",
        "\n",
        "\n",
        "docs = train.Complaint_processed.values.tolist()\n",
        "data_words = [tokenize(doc) for doc in docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQQfLhmdBkrS"
      },
      "outputs": [],
      "source": [
        "print(len(data_words))\n",
        "\n",
        "data_words[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE3TUdq8YcoI"
      },
      "source": [
        "## Convert Text into Numerical Representation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGS15M0-DGlj"
      },
      "outputs": [],
      "source": [
        "# In Natural language Processing there are many vectorizer which can convert text tokens to numerical tokens here we will try only two method \n",
        "# 1 - TF-IFD vectorizer    \n",
        "# 2 - Count Vectorizer\n",
        "# You can  find more about them in Document file\n",
        "\n",
        "tf_ifd  = TfidfVectorizer(tokenizer=tokenize, max_df = 0.75, min_df=50 , max_features=10000,use_idf= False,  lowercase=False )\n",
        "\n",
        "# Converting text into numerical representation\n",
        "cv = CountVectorizer(tokenizer=tokenize, max_df = 0.75, min_df=50 , max_features=10000, lowercase=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tLdhMEqaEjh"
      },
      "outputs": [],
      "source": [
        "tf_vector = tf_ifd.fit_transform(train.Complaint_processed)\n",
        "vc_vectore = cv.fit_transform(train.Complaint_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40Ox1v0NbTrj"
      },
      "outputs": [],
      "source": [
        "print(\"tf_vector matrx\",tf_vector.A)\n",
        "tf_vector.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9LhTRD3bYGK"
      },
      "outputs": [],
      "source": [
        "print(\"count vectorizer matrix \",vc_vectore.A)\n",
        "vc_vectore.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp1pJHWE5o9a"
      },
      "outputs": [],
      "source": [
        "cv_voc = (cv.get_feature_names())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ve_Bscbc6YX4"
      },
      "outputs": [],
      "source": [
        "tf_voc = tf_ifd.get_feature_names()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3lOA1PX748M"
      },
      "outputs": [],
      "source": [
        "print(\"length of tf_ifd vector vocabalury is :\",len(tf_ifd.get_feature_names()))\n",
        "print(\"length of cv vector vocabalury is :\",len(cv.get_feature_names()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16miJT5O8Wo_"
      },
      "source": [
        "## LDA Implementation Using Sckit learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkAKyT8n9c4X"
      },
      "outputs": [],
      "source": [
        "n_topics = 8\n",
        "model_lda = LDA(n_components = n_topics, learning_method='online',max_iter = 20,learning_offset=50,n_jobs = -1, random_state = 42)\n",
        "# fit transform on model on our count_vectorizer : running this will return our topics \n",
        "X_topics = model_lda.fit_transform(tf_vector)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr-gd2z41myi"
      },
      "outputs": [],
      "source": [
        "\n",
        "# .components_ gives us our topic distribution \n",
        "topic_words = model_lda.components_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtUA2_wFzzYg"
      },
      "outputs": [],
      "source": [
        "X_topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1G3sV5nz3sY"
      },
      "outputs": [],
      "source": [
        "topic_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIXlGiLT0WFG"
      },
      "source": [
        "So, **what X_topics and topic_words represents ?**\n",
        "\n",
        "\n",
        "*  **X_topics**: represents probability of each topic to be assigned to each document. So, it should be (no. of doc , no. of topics) shape.It is known as Document-topic matrix.\n",
        "*   **topic_words**: represents probability of particualr word being in particular topic. So, it is (no. of topics, no. of words) shape.It is known as Topic-Word matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0gpR34L0JNY"
      },
      "outputs": [],
      "source": [
        "print(topic_words.shape)\n",
        "print(X_topics.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHQCqKnD9qsa"
      },
      "outputs": [],
      "source": [
        "n_top_words = 15\n",
        "topics = []\n",
        "\n",
        "for i, topic_dist in enumerate(topic_words):\n",
        "    \n",
        "    # np.argsort to sorting an array or a list or the matrix acc to their values\n",
        "    sorted_topic_dist = np.argsort(topic_dist)\n",
        "    \n",
        "    # Next, to view the actual words present in those indexes we can make the use of the vocab created earlier\n",
        "    topic_words = np.array(tf_voc)[sorted_topic_dist]\n",
        "    \n",
        "    # so using the sorted_topic_indexes we are extracting the words from the vocabulary\n",
        "    # obtaining topics + words\n",
        "    # this topic_words variable contains the Topics  as well as the respective words present in those Topics\n",
        "    topic_words = topic_words[:-n_top_words:-1]\n",
        "    topics.append(topic_words)\n",
        "    print (\"Topic\", str(i), topic_words)\n",
        "topics=np.array(topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEpFQQKm-I4Z"
      },
      "outputs": [],
      "source": [
        "doc_topic = model_lda.transform(tf_vector)  \n",
        "topic_to_doc = []\n",
        "# iterating over ever value till the end value\n",
        "for n in range(doc_topic.shape[0]):\n",
        "    \n",
        "    # argmax() gives maximum index value\n",
        "    topic_doc = doc_topic[n].argmax()\n",
        "    topic_to_doc.append(topic_doc)\n",
        "\n",
        "def doc_to_topic(doc_no,topic_to_doc=topic_to_doc):\n",
        "  print(\"topic assigned to document \",doc_no,\" is \",topic_to_doc[doc_no+1],\" that is \",topics[topic_to_doc[doc_no+1]])\n",
        "\n",
        "doc_to_topic(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEHempuzF7ll"
      },
      "outputs": [],
      "source": [
        "topic_assigned_to_doc = pd.DataFrame(X_topics,columns=['topic0','topic1','topic2','topic3','topic4','topic5','topic6','topic7'])\n",
        "topic_assigned_to_doc['topic_assigned'] = topic_to_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bPLBmRmF-F9"
      },
      "outputs": [],
      "source": [
        "topic_assigned_to_doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWhpChujE3OY"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "km = KMeans(8,init='k-means++',max_iter=20)\n",
        "km.fit(tf_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_1m0tnDF39I"
      },
      "outputs": [],
      "source": [
        "centroids = km.cluster_centers_.argsort()[:,::-1]\n",
        "for i in range(8):\n",
        "  print(\"centroid \", i)\n",
        "  for ind in centroids[i,:15]:\n",
        "    print(tf_voc[ind],end=' ')\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQseKU9jJhJN"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "MNB = MultinomialNB()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHSZIVFOka2o"
      },
      "source": [
        "Non-negative Matrix Factorization (NNMF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFSvaatEkfpG"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import NMF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8qeMUW8kgox"
      },
      "outputs": [],
      "source": [
        "tf_ifd  = TfidfVectorizer(tokenizer=tokenize, max_df = 0.75, min_df=50 , max_features=10000,use_idf= False,  lowercase=False )\n",
        "X = tf_ifd.fit_transform(complaint_data.Complaint)\n",
        "words = np.array(tf_ifd.get_feature_names())\n",
        "\n",
        "print(X)\n",
        "print(\"X = \", words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egNXzd1-nyEb"
      },
      "outputs": [],
      "source": [
        "# Applying Non-Negative Matrix Factorization\n",
        " \n",
        "model_nmf = NMF(n_components=8, solver=\"mu\")\n",
        "W = model_nmf.fit_transform(X)\n",
        "H = model_nmf.components_\n",
        "\n",
        "for i, topic in enumerate(H):\n",
        "     print(\"Topic {}: {}\".format(i + 1, \",\".join([str(x) for x in words[topic.argsort()[-10:]]])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-ba1LAfqDr9"
      },
      "outputs": [],
      "source": [
        "doc_topic_byNMF = model_nmf.transform(X)  \n",
        "topic_to_doc_byNMF = []\n",
        "# iterating over ever value till the end value\n",
        "for n in range(doc_topic_byNMF.shape[0]):\n",
        "    \n",
        "    # argmax() gives maximum index value\n",
        "    topic_doc = doc_topic_byNMF[n].argmax()\n",
        "    topic_to_doc_byNMF.append(topic_doc)\n",
        "\n",
        "def doc_to_topic_byNMF(doc_no,topic_to_doc_byNMF=topic_to_doc_byNMF):\n",
        "  print(\"topic assigned to document \",doc_no,\" is \",topic_to_doc_byNMF[doc_no+1],\" that is \",topics[topic_to_doc_byNMF[doc_no+1]])\n",
        "\n",
        "doc_to_topic_byNMF(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXDV294urxow"
      },
      "outputs": [],
      "source": [
        "topic_assigned_to_doc_byNMF = pd.DataFrame(W,columns=['topic0','topic1','topic2','topic3','topic4','topic5','topic6','topic7'])\n",
        "topic_assigned_to_doc_byNMF['topic_assigned'] = topic_to_doc_byNMF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6BGVBRBsXCl"
      },
      "outputs": [],
      "source": [
        "topic_assigned_to_doc_byNMF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa7_Fa9c73Vt"
      },
      "source": [
        "Latent Semantic Analysis (LSA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwQpOznqszEX"
      },
      "outputs": [],
      "source": [
        "from scipy import linalg, spatial\n",
        "from sklearn.decomposition import PCA, SparsePCA, TruncatedSVD\n",
        "from sklearn.feature_extraction.text import (CountVectorizer, TfidfTransformer, TfidfVectorizer)\n",
        "\n",
        "from sklearn.utils.extmath import randomized_svd\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91yHQns2s9v4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
